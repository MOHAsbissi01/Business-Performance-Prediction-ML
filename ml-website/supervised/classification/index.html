<!DOCTYPE html>
<html lang="fr">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Classification Approfondie ‚Äî Profit_Class | Global Superstore</title>

  <link rel="stylesheet" href="../../style.css" />
  <link rel="stylesheet" href="../supervised.css" />
  <link rel="stylesheet" href="classification.css" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap"
    rel="stylesheet">

  <style>
    body {
      background: #0a0e27;
      color: #e4e8f0;
      font-family: 'Inter', sans-serif;
      line-height: 1.8;
    }

    .container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .hero-section {
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
      padding: 80px 0;
      text-align: center;
      color: white;
      margin-bottom: 60px;
    }

    .hero-section h1 {
      font-size: 3.5rem;
      margin-bottom: 1rem;
      font-weight: 800;
    }

    .hero-section .lead {
      font-size: 1.3rem;
      opacity: 0.95;
      max-width: 800px;
      margin: 0 auto 20px;
    }

    .hero-stats {
      display: flex;
      justify-content: center;
      gap: 40px;
      margin-top: 40px;
      flex-wrap: wrap;
    }

    .hero-stats .stat {
      text-align: center;
    }

    .hero-stats .stat h3 {
      font-size: 2.5rem;
      margin: 0;
      font-weight: 700;
    }

    .hero-stats .stat p {
      margin: 5px 0 0;
      opacity: 0.9;
    }

    .section {
      background: rgba(20, 27, 61, 0.6);
      border-radius: 20px;
      padding: 50px;
      margin-bottom: 40px;
      border: 1px solid rgba(240, 93, 251, 0.2);
    }

    .section h2 {
      color: #f0abfc;
      font-size: 2.2rem;
      margin-bottom: 1.5rem;
      border-bottom: 3px solid #f093fb;
      padding-bottom: 15px;
    }

    .section h3 {
      color: #fb923c;
      font-size: 1.8rem;
      margin-top: 2.5rem;
      margin-bottom: 1.2rem;
    }

    .section h4 {
      color: #fcd34d;
      font-size: 1.4rem;
      margin-top: 2rem;
      margin-bottom: 1rem;
    }

    .methodology-box {
      background: linear-gradient(135deg, rgba(240, 147, 251, 0.15), rgba(245, 87, 108, 0.15));
      border-left: 5px solid #f093fb;
      padding: 30px;
      border-radius: 12px;
      margin: 30px 0;
    }

    .formula {
      background: rgba(0, 0, 0, 0.3);
      padding: 20px 30px;
      border-radius: 12px;
      font-family: 'Courier New', monospace;
      font-size: 1.1rem;
      margin: 20px 0;
      border: 2px solid #f093fb;
      text-align: center;
      color: #f0abfc;
    }

    .insight-box {
      background: linear-gradient(135deg, rgba(96, 165, 250, 0.12), rgba(240, 147, 251, 0.12));
      border-left: 5px solid #60a5fa;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
      font-size: 1.05rem;
    }

    .insight-box strong {
      color: #60a5fa;
    }

    .success-box {
      background: linear-gradient(135deg, rgba(52, 211, 153, 0.15), rgba(16, 185, 129, 0.15));
      border-left: 5px solid #34d399;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
    }

    .success-box strong {
      color: #34d399;
    }

    .warning-box {
      background: linear-gradient(135deg, rgba(251, 191, 36, 0.15), rgba(245, 158, 11, 0.15));
      border-left: 5px solid #fbbf24;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
    }

    .warning-box strong {
      color: #fbbf24;
    }

    .results-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin: 30px 0;
    }

    .metric-card {
      background: rgba(240, 147, 251, 0.1);
      border: 2px solid #f093fb;
      padding: 25px;
      border-radius: 16px;
      text-align: center;
      transition: transform 0.3s, box-shadow 0.3s;
    }

    .metric-card:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 30px rgba(240, 147, 251, 0.4);
    }

    .metric-card h4 {
      color: #fcd34d;
      font-size: 0.95rem;
      margin: 0 0 10px;
      text-transform: uppercase;
    }

    .metric-card .value {
      font-size: 2.5rem;
      font-weight: 800;
      color: #f0abfc;
    }

    .metric-card.best {
      border-color: #34d399;
      background: rgba(52, 211, 153, 0.15);
    }

    .metric-card.best .value {
      color: #34d399;
    }

    .image-wrapper {
      margin: 40px 0;
      border-radius: 16px;
      overflow: hidden;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5);
    }

    .image-wrapper img {
      width: 100%;
      display: block;
    }

    .image-caption {
      text-align: center;
      margin-top: 15px;
      font-style: italic;
      color: #9ca3af;
      font-size: 0.95rem;
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 30px;
      margin: 40px 0;
    }

    .step-number {
      display: inline-block;
      background: #f093fb;
      color: white;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      text-align: center;
      line-height: 40px;
      font-weight: 700;
      margin-right: 15px;
      font-size: 1.2rem;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 30px 0;
      background: rgba(20, 27, 61, 0.4);
      border-radius: 12px;
      overflow: hidden;
    }

    thead {
      background: rgba(240, 147, 251, 0.3);
    }

    th,
    td {
      padding: 15px;
      text-align: left;
      border-bottom: 1px solid rgba(240, 147, 251, 0.2);
    }

    th {
      color: #f0abfc;
      font-weight: 600;
    }

    tr:hover {
      background: rgba(240, 147, 251, 0.1);
    }

    .class-label {
      display: inline-block;
      padding: 6px 14px;
      border-radius: 20px;
      font-weight: 600;
      font-size: 0.95rem;
    }

    .class-low {
      background: rgba(239, 68, 68, 0.2);
      color: #fca5a5;
      border: 2px solid #ef4444;
    }

    .class-medium {
      background: rgba(251, 191, 36, 0.2);
      color: #fcd34d;
      border: 2px solid #fbbf24;
    }

    .class-high {
      background: rgba(52, 211, 153, 0.2);
      color: #6ee7b7;
      border: 2px solid #34d399;
    }

    code {
      background: rgba(240, 147, 251, 0.2);
      padding: 3px 8px;
      border-radius: 6px;
      color: #f0abfc;
      font-family: 'Courier New', monospace;
      font-size: 0.95em;
    }

    .nav-top {
      background: rgba(10, 14, 39, 0.95);
      padding: 20px 0;
      position: sticky;
      top: 0;
      z-index: 100;
      border-bottom: 1px solid rgba(240, 147, 251, 0.3);
    }

    .nav-top .container {
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .nav-top a {
      color: #f0abfc;
      text-decoration: none;
      transition: color 0.3s;
    }

    .nav-top a:hover {
      color: #fcd34d;
    }

    .process-flow {
      background: rgba(20, 27, 61, 0.6);
      border-radius: 16px;
      padding: 30px;
      margin: 30px 0;
    }

    .process-step {
      padding: 20px 0;
      border-left: 3px solid #f093fb;
      padding-left: 30px;
      margin-left: 20px;
      position: relative;
    }

    .process-step::before {
      content: '';
      position: absolute;
      left: -8px;
      top: 25px;
      width: 13px;
      height: 13px;
      border-radius: 50%;
      background: #f093fb;
      border: 3px solid #0a0e27;
    }
  </style>
</head>

<body>

  <!-- Navigation -->
  <nav class="nav-top">
    <div class="container">
      <a href="../index.html">‚Üê Apprentissage Supervis√©</a>
      <a href="../../index.html">üè† Accueil</a>
      <a href="../regression/index.html">R√©gression ‚Üí</a>
    </div>
  </nav>

  <!-- Hero -->
  <header class="hero-section">
    <div class="container">
      <h1>üéØ Classification Multi-Classes Approfondie</h1>
      <p class="lead">
        Pr√©dire la classe de profit d'une commande avec des algorithmes avanc√©s<br>
        <strong>Low ‚Ä¢ Medium ‚Ä¢ High ‚Äî Une approche comparative compl√®te</strong>
      </p>
      <div class="hero-stats">
        <div class="stat">
          <h3>51 290</h3>
          <p>Commandes class√©es</p>
        </div>
        <div class="stat">
          <h3>3 classes</h3>
          <p>Parfaitement √©quilibr√©es</p>
        </div>
        <div class="stat">
          <h3>74.3%</h3>
          <p>Accuracy champion (XGBoost)</p>
        </div>
        <div class="stat">
          <h3>23</h3>
          <p>Features s√©lectionn√©es</p>
        </div>
      </div>
    </div>
  </header>

  <div class="container">

    <!-- Introduction & Problem Definition -->
    <section class="section">
      <h2>üéØ Contexte et D√©finition du Probl√®me</h2>

      <p style="font-size: 1.15rem;">
        Alors que la r√©gression pr√©dit des <strong>valeurs continues</strong> (montant des ventes),
        la classification assigne des <strong>cat√©gories discr√®tes</strong>. Dans ce projet, nous cr√©ons
        une cible <code>Profit_Class</code> pour cat√©goriser chaque commande selon sa rentabilit√©.
      </p>

      <div class="methodology-box">
        <h4 style="color: #f093fb; margin-top: 0;">üè∑Ô∏è Cr√©ation de la Variable Cible</h4>
        <p>
          Nous avons segment√© la variable continue <code>Profit</code> en <strong>3 classes √©quilibr√©es</strong>
          en utilisant les tertiles (quantiles √† 33% et 67%) :
        </p>

        <div style="margin: 25px 0;">
          <div style="display: flex; align-items: center; margin: 15px 0;">
            <span class="class-label class-low">Low</span>
            <span style="margin-left: 20px;">Profit < $8.67 ‚Üí <strong>17 099 commandes</strong> (33.3%)</span>
          </div>
          <div style="display: flex; align-items: center; margin: 15px 0;">
            <span class="class-label class-medium">Medium</span>
            <span style="margin-left: 20px;">$8.67 ‚â§ Profit < $29.36 ‚Üí <strong>17 097 commandes</strong> (33.3%)</span>
          </div>
          <div style="display: flex; align-items: center; margin: 15px 0;">
            <span class="class-label class-high">High</span>
            <span style="margin-left: 20px;">Profit ‚â• $29.36 ‚Üí <strong>17 094 commandes</strong> (33.4%)</span>
          </div>
        </div>

        <p style="margin-top: 20px;">
          <strong>Split Train/Test :</strong> 80/20 stratifi√© ‚Üí Train = 41 032 | Test = 10 258<br>
          La stratification pr√©serve la distribution des classes dans les deux ensembles.
        </p>
      </div>

      <div class="insight-box">
        <strong>üíº Int√©r√™t Business :</strong><br><br>
        Identifier la classe de profit <em>avant</em> la finalisation d'une commande permet de :<br>
        ‚Ä¢ <strong>Prioriser les commandes High</strong> (traitement rapide, service premium)<br>
        ‚Ä¢ <strong>Optimiser les remises</strong> pour √©viter les commandes Low<br>
        ‚Ä¢ <strong>Segmenter les clients</strong> selon leur potentiel de rentabilit√©<br>
        ‚Ä¢ <strong>D√©tecter les patterns</strong> : quels produits/segments/r√©gions g√©n√®rent le plus de profit ?
      </div>

      <h3>üî¨ M√©thodologie et Algorithmes Test√©s</h3>

      <div class="process-flow">
        <div class="process-step">
          <strong style="color: #f0abfc;">√âtape 1 : Pr√©paration des donn√©es</strong><br>
          Encodage cat√©goriel (One-Hot), normalisation (StandardScaler), gestion des d√©s√©quilibres (ici √©quilibr√©)
        </div>
        <div class="process-step">
          <strong style="color: #f0abfc;">√âtape 2 : S√©lection des mod√®les</strong><br>
          KNN, SVM, Random Forest, XGBoost, CatBoost ‚Äî du plus simple au plus sophistiqu√©
        </div>
        <div class="process-step">
          <strong style="color: #f0abfc;">√âtape 3 : Entra√Ænement et √©valuation</strong><br>
          M√©triques : Accuracy, Precision, Recall, F1-Score, Confusion Matrix, ROC AUC
        </div>
        <div class="process-step">
          <strong style="color: #f0abfc;">√âtape 4 : Optimisation hyperparam√®tres</strong><br>
          GridSearchCV / Hyperopt pour fine-tuning (CatBoost)
        </div>
        <div class="process-step">
          <strong style="color: #f0abfc;">√âtape 5 : S√©lection du champion</strong><br>
          Comparaison finale bas√©e sur Accuracy, F1-macro, et robustesse
        </div>
      </div>
    </section>

    <!-- Global Results Comparison -->
    <section class="section">
      <h2>üìä Comparaison Globale des Mod√®les</h2>

      <h3>üèÜ Top 5 ‚Äî Classement des Performances</h3>

      <table>
        <thead>
          <tr>
            <th>Rang</th>
            <th>Mod√®le</th>
            <th>Accuracy Test</th>
            <th>F1-Score Macro</th>
            <th>Pr√©cision Moyenne</th>
            <th>Rappel Moyen</th>
          </tr>
        </thead>
        <tbody>
          <tr style="background: rgba(52, 211, 153, 0.15);">
            <td><strong>ü•á 1</strong></td>
            <td><strong>XGBoost</strong></td>
            <td><strong>74.33%</strong></td>
            <td><strong>0.74</strong></td>
            <td>74%</td>
            <td>74%</td>
          </tr>
          <tr style="background: rgba(52, 211, 153, 0.12);">
            <td><strong>ü•à 2</strong></td>
            <td><strong>CatBoost (optimis√©)</strong></td>
            <td><strong>74.30%</strong></td>
            <td><strong>0.74</strong></td>
            <td>74%</td>
            <td>74%</td>
          </tr>
          <tr>
            <td><strong>ü•â 3</strong></td>
            <td>Random Forest</td>
            <td>73.0%</td>
            <td>0.73</td>
            <td>73%</td>
            <td>73%</td>
          </tr>
          <tr>
            <td>4</td>
            <td>SVM (RBF Kernel)</td>
            <td>72.5%</td>
            <td>0.73</td>
            <td>72%</td>
            <td>73%</td>
          </tr>
          <tr>
            <td>5</td>
            <td>K-Nearest Neighbors</td>
            <td>65.3%</td>
            <td>0.66</td>
            <td>66%</td>
            <td>65%</td>
          </tr>
        </tbody>
      </table>

      <div class="insight-box">
        <strong>üìà Observations Cl√©s :</strong><br><br>
        ‚Ä¢ <strong>Domination des Gradient Boosting</strong> : XGBoost et CatBoost se partagent la premi√®re place √†
        ~74.3%<br>
        ‚Ä¢ <strong>Random Forest solide</strong> : 73% confirme la puissance des ensembles d'arbres<br>
        ‚Ä¢ <strong>SVM comp√©titif</strong> : malgr√© un co√ªt computationnel √©lev√© (kernel RBF), atteint 72.5%<br>
        ‚Ä¢ <strong>KNN en retrait</strong> : 65.3% sugg√®re que les fronti√®res de d√©cision sont complexes et
        non-locales<br>
        ‚Ä¢ <strong>F1-Score = Accuracy</strong> : confirme que le dataset est bien √©quilibr√© (pas de biais de classe)
      </div>
    </section>

    <!-- Model 1: XGBoost Champion -->
    <section class="section">
      <h2><span class="step-number">ü•á</span> XGBoost ‚Äî Mod√®le Champion</h2>

      <h3>‚öôÔ∏è Principe du Gradient Boosting</h3>
      <p>
        XGBoost (eXtreme Gradient Boosting) construit <strong>s√©quentiellement des arbres de d√©cision</strong>,
        o√π chaque nouvel arbre corrige les erreurs du pr√©c√©dent. C'est un algorithme de <em>boosting</em>,
        contrairement au <em>bagging</em> de Random Forest.
      </p>

      <div class="methodology-box">
        <h4 style="color: #f093fb; margin-top: 0;">üî¨ Fonctionnement du Boosting</h4>
        <ol style="line-height: 2;">
          <li><strong>Initialisation</strong> : Pr√©diction triviale (moyenne ou classe majoritaire)</li>
          <li><strong>It√©ration 1</strong> : Construction d'un arbre qui mod√©lise les <em>r√©sidus</em> (erreurs) de
            l'it√©ration pr√©c√©dente</li>
          <li><strong>Mise √† jour</strong> : Pr√©diction = Pr√©diction pr√©c√©dente + Œ± √ó Nouvel arbre</li>
          <li><strong>R√©p√©tition</strong> : N arbres sont construits, chacun se concentrant sur les erreurs r√©siduelles
          </li>
          <li><strong>R√©gularisation</strong> : P√©nalit√©s L1/L2 sur les feuilles pour √©viter l'overfitting</li>
        </ol>

        <p style="margin-top: 20px;">
          <strong>Avantages cl√©s de XGBoost :</strong><br>
          ‚Ä¢ <strong>Gestion native des features cat√©gorielles</strong> (apr√®s encodage)<br>
          ‚Ä¢ <strong>Robustesse aux outliers</strong> gr√¢ce aux arbres de d√©cision<br>
          ‚Ä¢ <strong>Parall√©lisation efficace</strong> pour des performances rapides<br>
          ‚Ä¢ <strong>Importances des features</strong> pour l'interpr√©tabilit√©
        </p>
      </div>

      <h3>üìä Performances D√©taill√©es</h3>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>Accuracy</h4>
          <div class="value">74.33%</div>
        </div>
        <div class="metric-card best">
          <h4>F1-Score Macro</h4>
          <div class="value">0.74</div>
        </div>
        <div class="metric-card best">
          <h4>PR AUC Macro</h4>
          <div class="value">0.80</div>
        </div>
        <div class="metric-card best">
          <h4>ROC AUC Macro</h4>
          <div class="value">0.88</div>
        </div>
      </div>

      <div class="image-wrapper">
        <img src="images/XGboost.png" alt="Matrice de confusion et m√©triques XGBoost" />
        <p class="image-caption">Figure 1 : Confusion Matrix et Classification Report ‚Äî XGBoost</p>
      </div>

      <h3>üîç Analyse Par Classe</h3>

      <table>
        <thead>
          <tr>
            <th>Classe</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1-Score</th>
            <th>Support (Test)</th>
            <th>Interpr√©tation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><span class="class-label class-low">Low</span></td>
            <td>87%</td>
            <td>59%</td>
            <td>0.70</td>
            <td>3 418</td>
            <td>Tr√®s pr√©cis mais manque de sensibilit√©</td>
          </tr>
          <tr>
            <td><span class="class-label class-medium">Medium</span></td>
            <td>61%</td>
            <td>71%</td>
            <td>0.66</td>
            <td>3 418</td>
            <td>Bon rappel, souvent confondu avec Low/High</td>
          </tr>
          <tr>
            <td><span class="class-label class-high">High</span></td>
            <td>76%</td>
            <td>81%</td>
            <td>0.78</td>
            <td>3 422</td>
            <td>Meilleure performance globale</td>
          </tr>
        </tbody>
      </table>

      <div class="insight-box">
        <strong>üí° Interpr√©tation D√©taill√©e :</strong><br><br>

        <strong>Classe Low (F1 = 0.70) :</strong><br>
        ‚Ä¢ <strong>Precision = 87%</strong> ‚Üí Quand le mod√®le pr√©dit "Low", il a 87% de chances d'√™tre correct<br>
        ‚Ä¢ <strong>Recall = 59%</strong> ‚Üí Mais il manque 41% des vraies commandes "Low" (class√©es √† tort en Medium)<br>
        ‚Ä¢ <strong>Explication :</strong> La fronti√®re entre Low et Medium est floue (seuils de profit proches)<br><br>

        <strong>Classe Medium (F1 = 0.66) :</strong><br>
        ‚Ä¢ C'est la classe la plus difficile ‚Äî elle est <em>√©cras√©e</em> entre Low et High<br>
        ‚Ä¢ Precision mod√©r√©e (61%) : beaucoup de faux positifs (Low/High class√©s comme Medium)<br>
        ‚Ä¢ Recall correct (71%) : d√©tecte bien les vrais Medium<br><br>

        <strong>Classe High (F1 = 0.78) ‚≠ê :</strong><br>
        ‚Ä¢ <strong>Meilleure performance</strong> : les commandes tr√®s rentables ont des caract√©ristiques
        distinctives<br>
        ‚Ä¢ Recall √©lev√© (81%) : essentiel pour ne pas manquer les opportunit√©s √† fort profit !
      </div>

      <div class="success-box">
        <strong>‚úÖ Pourquoi XGBoost est Champion :</strong><br><br>
        ‚Ä¢ <strong>74.3% d'Accuracy</strong> sur un probl√®me multi-classes √©quilibr√© est excellent<br>
        ‚Ä¢ <strong>ROC AUC = 0.88</strong> ‚Üí excellente capacit√© de discrimination<br>
        ‚Ä¢ <strong>Classe High bien d√©tect√©e</strong> (81% recall) ‚Üí crucial pour prioriser les commandes rentables<br>
        ‚Ä¢ <strong>Temps d'entra√Ænement rapide</strong> (~5s) gr√¢ce √† la parall√©lisation<br>
        ‚Ä¢ <strong>Feature importances exploitables</strong> pour l'interpr√©tabilit√© business
      </div>
    </section>

    <!-- Model 2: CatBoost -->
    <section class="section">
      <h2><span class="step-number">ü•à</span> CatBoost ‚Äî Vice-Champion</h2>

      <h3>üê± Sp√©cificit√©s de CatBoost</h3>
      <p>
        CatBoost (Categorical Boosting) est un algorithme de gradient boosting d√©velopp√© par Yandex,
        sp√©cialis√© dans le <strong>traitement natif des variables cat√©gorielles</strong> sans encodage manuel.
      </p>

      <div class="methodology-box">
        <h4 style="color: #f093fb; margin-top: 0;">üîß Optimisation Hyperparam√®tres (Hyperopt)</h4>
        <p>
          Nous avons utilis√© <strong>Hyperopt</strong> pour optimiser automatiquement les hyperparam√®tres :
        </p>
        <ul style="line-height: 2;">
          <li><code>iterations</code> : Nombre d'arbres (100-500)</li>
          <li><code>depth</code> : Profondeur max des arbres (4-10)</li>
          <li><code>learning_rate</code> : Taux d'apprentissage (0.01-0.3)</li>
          <li><code>l2_leaf_reg</code> : R√©gularisation L2 sur les feuilles</li>
        </ul>

        <p style="margin-top: 15px;">
          <strong>R√©sultat :</strong> Apr√®s optimisation, CatBoost atteint <strong>74.30% d'accuracy</strong>,
          √©galant pratiquement XGBoost.
        </p>
      </div>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>Accuracy</h4>
          <div class="value">74.30%</div>
        </div>
        <div class="metric-card best">
          <h4>F1-Score Macro</h4>
          <div class="value">0.74</div>
        </div>
        <div class="metric-card">
          <h4>Temps Train</h4>
          <div class="value" style="font-size: 1.8rem;">~8s</div>
        </div>
        <div class="metric-card">
          <h4>Hyperopt Trials</h4>
          <div class="value" style="font-size: 1.8rem;">50</div>
        </div>
      </div>

      <div class="image-wrapper">
        <img src="images/catboost.png" alt="CatBoost classification report" />
        <p class="image-caption">Figure 2 : Performances CatBoost apr√®s optimisation</p>
      </div>

      <div class="image-wrapper">
        <img src="images/boxplot-catboost.png" alt="Feature importances CatBoost" />
        <p class="image-caption">Figure 3 : Feature Importances ‚Äî Top 15 features contributives</p>
      </div>

      <div class="image-wrapper">
        <img src="images/CatboostFeatureImportance.png" alt="CatBoost Feature Importance Analysis" />
        <p class="image-caption">Figure 4 : Analyse D√©taill√©e des Features Importantes ‚Äî CatBoost</p>
      </div>

      <h3>üìä Analyse du Recall par Classe</h3>

      <div class="image-wrapper">
        <img src="images/CatboostReacall.png" alt="CatBoost Recall par Classe" />
        <p class="image-caption">Figure 5 : Recall Initial par Classe ‚Äî CatBoost Baseline</p>
      </div>

      <div class="image-wrapper">
        <img src="images/CatboostReacallOptimise.png" alt="CatBoost Recall Optimis√©" />
        <p class="image-caption">Figure 6 : Recall par Classe Apr√®s Optimisation Hyperopt ‚Äî Am√©lioration de la
          Sensibilit√©</p>
      </div>

      <div class="methodology-box">
        <h4 style="color: #f093fb; margin-top: 0;">üìà Impact de l'Optimisation sur le Recall</h4>
        <p>
          L'optimisation via <strong>Hyperopt</strong> a permis d'am√©liorer significativement le <strong>recall</strong>
          pour chaque classe, particuli√®rement pour la classe Medium qui √©tait initialement la plus difficile √†
          d√©tecter.
        </p>
        <p style="margin-top: 15px;">
          <strong>Am√©lioration du Recall :</strong><br>
          ‚Ä¢ <strong>Classe Low :</strong> Meilleure d√©tection des commandes peu rentables<br>
          ‚Ä¢ <strong>Classe Medium :</strong> R√©duction des confusions avec Low et High<br>
          ‚Ä¢ <strong>Classe High :</strong> Maintien d'un excellent taux de d√©tection des commandes rentables<br><br>
          Cette am√©lioration du recall signifie que le mod√®le optimis√© <em>manque moins de vraies instances</em>
          de chaque classe, essentiel pour des d√©cisions business fiables.
        </p>
      </div>

      <div class="insight-box">
        <strong>üéØ Features les Plus Importantes (CatBoost) :</strong><br><br>
        1. <strong>Sales</strong> : Sans surprise, le montant des ventes est fortement corr√©l√© au profit<br>
        2. <strong>Discount</strong> : Impact majeur ‚Äî remises √©lev√©es ‚Üí profit faible<br>
        3. <strong>Quantity</strong> : Volume command√© influence directement la rentabilit√©<br>
        4. <strong>Shipping Cost</strong> : Co√ªts d'exp√©dition √©lev√©s r√©duisent le profit net<br>
        5. <strong>Segment (Consumer/Corporate/Home Office)</strong> : Certains segments sont plus rentables<br>
        6. <strong>Category (Furniture/Office Supplies/Technology)</strong> : Marges variables selon la
        cat√©gorie<br><br>

        Ces insights permettent d'<strong>orienter la strat√©gie business</strong> : limiter les discount excessifs,
        optimiser le shipping, cibler les segments rentables.
      </div>
    </section>

    <!-- Model 3: Random Forest -->
    <section class="section">
      <h2><span class="step-number">ü•â</span> Random Forest ‚Äî M√©daille de Bronze</h2>

      <h3>üå≤ Principe des For√™ts Al√©atoires</h3>
      <p>
        Random Forest est un algorithme d'<strong>ensemble learning</strong> bas√© sur le <em>bagging</em> :
        construction de multiples arbres de d√©cision sur des √©chantillons al√©atoires des donn√©es,
        puis agr√©gation des pr√©dictions par vote majoritaire.
      </p>

      <div class="results-grid">
        <div class="metric-card">
          <h4>Accuracy</h4>
          <div class="value">73.0%</div>
        </div>
        <div class="metric-card">
          <h4>F1-Score Macro</h4>
          <div class="value">0.73</div>
        </div>
        <div class="metric-card">
          <h4>Nombre d'Arbres</h4>
          <div class="value" style="font-size: 2rem;">100</div>
        </div>
        <div class="metric-card">
          <h4>Temps Train</h4>
          <div class="value" style="font-size: 1.8rem;">~12s</div>
        </div>
      </div>

      <div class="image-wrapper">
        <img src="images/Confusion Matrix RandomForest.png" alt="Random Forest confusion matrix" />
        <p class="image-caption">Figure 4 : Matrice de Confusion ‚Äî Random Forest</p>
      </div>

      <div class="insight-box">
        <strong>‚úÖ Points Forts :</strong><br>
        ‚Ä¢ <strong>Robustesse</strong> : Peu sensible aux outliers et au bruit<br>
        ‚Ä¢ <strong>Pas d'overfitting</strong> : La randomisation naturelle √©vite le surapprentissage<br>
        ‚Ä¢ <strong>Importances des features</strong> : Facile √† interpr√©ter<br><br>

        <strong>‚ö†Ô∏è Limites :</strong><br>
        ‚Ä¢ <strong>L√©g√®rement moins performant</strong> que les boosting (73% vs 74.3%)<br>
        ‚Ä¢ <strong>Temps d'entra√Ænement plus long</strong> (~12s vs ~5s pour XGBoost)<br>
        ‚Ä¢ Ne capture pas aussi bien les interactions complexes entre features
      </div>
    </section>

    <!-- Model 4: SVM -->
    <section class="section">
      <h2><span class="step-number">4</span> SVM (RBF Kernel) ‚Äî 72.5%</h2>

      <h3>üîÆ Support Vector Machine avec Kernel RBF</h3>
      <p>
        SVM cherche √† trouver un <strong>hyperplan optimal</strong> qui s√©pare les classes avec la marge maximale.
        Le <strong>kernel RBF (Radial Basis Function)</strong> permet de projeter les donn√©es dans un espace
        de dimension sup√©rieure pour g√©rer des fronti√®res non-lin√©aires.
      </p>

      <div class="formula">
        K(x, x') = exp(-Œ≥ ||x - x'||¬≤)
      </div>

      <div class="results-grid">
        <div class="metric-card">
          <h4>Accuracy</h4>
          <div class="value">72.5%</div>
        </div>
        <div class="metric-card">
          <h4>F1-Score Macro</h4>
          <div class="value">0.73</div>
        </div>
        <div class="metric-card">
          <h4>Kernel</h4>
          <div class="value" style="font-size: 1.5rem;">RBF</div>
        </div>
        <div class="metric-card">
          <h4>Temps Train</h4>
          <div class="value" style="font-size: 1.5rem;">~45s ‚ö†Ô∏è</div>
        </div>
      </div>

      <div class="image-wrapper">
        <img src="images/SVM.png" alt="SVM metrics" />
        <p class="image-caption">Figure 5 : Performance SVM avec kernel RBF</p>
      </div>

      <div class="warning-box">
        <strong>‚ö†Ô∏è SVM ‚Äî Trade-off Performance/Co√ªt :</strong><br><br>
        ‚Ä¢ <strong>72.5% d'accuracy</strong> est honorable, mais inf√©rieur aux boosting<br>
        ‚Ä¢ <strong>Temps d'entra√Ænement ~45s</strong> ‚Üí impraticable pour de tr√®s gros datasets<br>
        ‚Ä¢ <strong>Complexit√© O(n¬≤) √† O(n¬≥)</strong> pour le calcul de la matrice kernel<br>
        ‚Ä¢ Meilleur sur des datasets plus petits ou avec des fronti√®res de d√©cision tr√®s complexes<br><br>

        <strong>Verdict :</strong> SVM reste comp√©titif mais <em>moins scalable</em> que les algorithmes
        bas√©s sur les arbres pour ce type de probl√®me.
      </div>
    </section>

    <!-- Model 5: KNN -->
    <section class="section">
      <h2><span class="step-number">5</span> K-Nearest Neighbors ‚Äî 65.3%</h2>

      <h3>üë• Classification par Voisinage</h3>
      <p>
        KNN est un algorithme <strong>non-param√©trique</strong> qui classe un point en fonction des
        <code>K</code> points les plus proches dans l'espace des features.
      </p>

      <div class="results-grid">
        <div class="metric-card">
          <h4>Accuracy</h4>
          <div class="value">65.3%</div>
        </div>
        <div class="metric-card">
          <h4>F1-Score Macro</h4>
          <div class="value">0.66</div>
        </div>
        <div class="metric-card">
          <h4>K Optimal</h4>
          <div class="value" style="font-size: 2rem;">5</div>
        </div>
        <div class="metric-card">
          <h4>Distance</h4>
          <div class="value" style="font-size: 1.5rem;">Euclidienne</div>
        </div>
      </div>

      <div class="image-wrapper">
        <img src="images/KNN.png" alt="KNN performance" />
        <p class="image-caption">Figure 6 : Matrice de confusion KNN (K=5)</p>
      </div>

      <div class="warning-box">
        <strong>‚ùå Pourquoi KNN Est en Retrait :</strong><br><br>
        ‚Ä¢ <strong>65.3% seulement</strong> ‚Üí √©cart de -9% avec XGBoost<br>
        ‚Ä¢ <strong>Hypoth√®se de voisinage local</strong> : ne fonctionne bien que si les classes sont
        spatialement group√©es. Ici, les fronti√®res sont complexes et non-locales<br>
        ‚Ä¢ <strong>Sensible √† la dimension</strong> : avec 23 features, la "mal√©diction de la dimensionnalit√©"
        rend les distances moins discriminantes<br>
        ‚Ä¢ <strong>Pas d'apprentissage</strong> : simple m√©morisation, pas de capture de patterns globaux<br><br>

        <strong>Conclusion :</strong> KNN est utile pour des probl√®mes simples √† faible dimension,
        mais inadapt√© ici face aux algorithmes modernes.
      </div>
    </section>

    <!-- Conclusion & Insights -->
    <section class="section">
      <h2>üèÅ Synth√®se et Conclusion</h2>

      <h3>üìä Tableau R√©capitulatif Final</h3>

      <table>
        <thead>
          <tr>
            <th>Mod√®le</th>
            <th>Accuracy</th>
            <th>F1-Macro</th>
            <th>Temps Train</th>
            <th>Avantages</th>
            <th>Limites</th>
          </tr>
        </thead>
        <tbody>
          <tr style="background: rgba(52, 211, 153, 0.15);">
            <td><strong>XGBoost</strong></td>
            <td><strong>74.33%</strong></td>
            <td><strong>0.74</strong></td>
            <td>~5s</td>
            <td>Rapide, performant, interpr√©table</td>
            <td>Hyperparam√®tres sensibles</td>
          </tr>
          <tr style="background: rgba(52, 211, 153, 0.12);">
            <td><strong>CatBoost</strong></td>
            <td><strong>74.30%</strong></td>
            <td><strong>0.74</strong></td>
            <td>~8s</td>
            <td>Cat√©gorielles natives, robuste</td>
            <td>L√©g√®rement plus lent</td>
          </tr>
          <tr>
            <td>Random Forest</td>
            <td>73.0%</td>
            <td>0.73</td>
            <td>~12s</td>
            <td>Robuste, peu d'overfitting</td>
            <td>Moins performant que boosting</td>
          </tr>
          <tr>
            <td>SVM (RBF)</td>
            <td>72.5%</td>
            <td>0.73</td>
            <td>~45s</td>
            <td>Fronti√®res complexes</td>
            <td>Co√ªt computationnel √©lev√©</td>
          </tr>
          <tr>
            <td>KNN</td>
            <td>65.3%</td>
            <td>0.66</td>
            <td>~1s</td>
            <td>Simple, pas d'entra√Ænement</td>
            <td>Sensible √† la dimension</td>
          </tr>
        </tbody>
      </table>

      <div class="success-box">
        <h3 style="color: #34d399; margin-top: 0;">üéì Enseignements Cl√©s</h3>

        <p><strong>1. Domination des Gradient Boosting :</strong></p>
        <p>
          XGBoost et CatBoost surpassent tous les autres algorithmes avec <strong>74.3% d'accuracy</strong>.
          Leur capacit√© √† capturer des interactions complexes entre features via des arbres s√©quentiels
          est id√©ale pour ce type de probl√®me.
        </p>

        <p style="margin-top: 20px;"><strong>2. Importance de l'Optimisation :</strong></p>
        <p>
          CatBoost non-optimis√© atteignait ~72%. L'<strong>optimisation Hyperopt</strong> a gagn√© +2.3%
          d'accuracy ‚Äî preuve que le fine-tuning des hyperparam√®tres est crucial pour maximiser les performances.
        </p>

        <p style="margin-top: 20px;"><strong>3. √âquilibre Classes ‚Üí Metrics Fiables :</strong></p>
        <p>
          Avec des classes parfaitement √©quilibr√©es (33.3% chacune), l'<strong>Accuracy = F1-Score</strong>.
          Pas de biais de classe √† g√©rer, les m√©triques refl√®tent fid√®lement la performance r√©elle.
        </p>

        <p style="margin-top: 20px;"><strong>4. Features Importance ‚Üí Insights Business :</strong></p>
        <ul style="line-height: 1.8;">
          <li><strong>Sales et Discount</strong> dominent les importances ‚Üí leviers strat√©giques prioritaires</li>
          <li><strong>Segment et Category</strong> r√©v√®lent des opportunit√©s de ciblage</li>
          <li><strong>Shipping Cost</strong> impacte directement la profitabilit√© ‚Üí optimisation logistique n√©cessaire
          </li>
        </ul>

        <p style="margin-top: 20px;"><strong>5. 74.3% ‚Äî Performance Excellente pour un Multi-Class :</strong></p>
        <p>
          Sur un probl√®me √† 3 classes, une accuracy >70% est consid√©r√©e comme <em>tr√®s bonne</em>.
          Atteindre 74.3% signifie que le mod√®le capture efficacement les patterns de rentabilit√©.
          Pour r√©f√©rence, une pr√©diction al√©atoire donnerait 33.3%.
        </p>
      </div>

      <div class="insight-box">
        <strong>üíº Applications Pratiques :</strong><br><br>

        <strong>1. Priorisation des Commandes :</strong><br>
        ‚Ä¢ Commandes pr√©dites <span class="class-label class-high">High</span> ‚Üí processing express, service premium<br>
        ‚Ä¢ Commandes pr√©dites <span class="class-label class-low">Low</span> ‚Üí investigation (discount excessif ? produit
        non rentable ?)<br><br>

        <strong>2. Optimisation des Remises :</strong><br>
        ‚Ä¢ Le mod√®le peut simuler l'impact d'un discount sur la classe de profit<br>
        ‚Ä¢ √âviter les remises qui font basculer une commande de High ‚Üí Low<br><br>

        <strong>3. Segmentation Client :</strong><br>
        ‚Ä¢ Identifier les clients qui g√©n√®rent majoritairement des commandes <span
          class="class-label class-high">High</span><br>
        ‚Ä¢ Programmes de fid√©lisation cibl√©s sur ces segments rentables<br><br>

        <strong>4. Alerting & Anomalies :</strong><br>
        ‚Ä¢ D√©tection de commandes atypiques (pr√©diction != r√©alit√©) ‚Üí possibles erreurs de saisie ou fraudes
      </div>

      <div class="methodology-box">
        <h4 style="color: #f093fb; margin-top: 0;">üöÄ Prochaines √âtapes</h4>
        <p><strong>1. Ensemble / Stacking :</strong></p>
        <p>
          Combiner XGBoost + CatBoost + Random Forest dans un meta-mod√®le pour potentiellement atteindre
          75-76% d'accuracy.
        </p>

        <p style="margin-top: 15px;"><strong>2. Feature Engineering Avanc√© :</strong></p>
        <ul style="line-height: 1.8;">
          <li>Agr√©gations temporelles (tendances mensuelles, saisonnalit√©)</li>
          <li>Features RFM (Recency, Frequency, Monetary) par client</li>
          <li>Interactions manuelles (Discount √ó Category, Sales / Quantity, etc.)</li>
        </ul>

        <p style="margin-top: 15px;"><strong>3. D√©ploiement Production :</strong></p>
        <p>
          API REST (FastAPI) pour pr√©diction en temps r√©el :<br>
          <code>POST /predict</code> ‚Üí retourne la classe de profit pr√©dite + probabilit√©s par classe
        </p>

        <p style="margin-top: 15px;"><strong>4. Monitoring & Drift Detection :</strong></p>
        <p>
          Surveillance continue des distributions de features et des performances pour d√©tecter
          tout changement dans les patterns de donn√©es (data drift).
        </p>
      </div>
    </section>

  </div>

  <!-- Footer -->
  <footer
    style="background: #0a0e27; border-top: 1px solid rgba(240, 147, 251, 0.3); padding: 40px 0; text-align: center; margin-top: 80px;">
    <div class="container">
      <p style="color: #9ca3af; font-size: 0.95rem;">
        ¬© 2025 Global Superstore ‚Äî Classification Multi-Classes Approfondie<br>
        <span style="color: #f093fb;">Machine Learning Portfolio Project</span>
      </p>
    </div>
  </footer>

</body>

</html>