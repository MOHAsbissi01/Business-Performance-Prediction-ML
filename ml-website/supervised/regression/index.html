<!DOCTYPE html>
<html lang="fr">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Analyse de R√©gression Approfondie | Global Superstore Sales</title>
  <link rel="stylesheet" href="../../style.css" />
  <link rel="stylesheet" href="regression.css" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap"
    rel="stylesheet">
  <style>
    body {
      background: #0a0e27;
      color: #e4e8f0;
      font-family: 'Inter', sans-serif;
      line-height: 1.8;
    }

    .container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .hero-section {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      padding: 80px 0;
      text-align: center;
      color: white;
      margin-bottom: 60px;
    }

    .hero-section h1 {
      font-size: 3.5rem;
      margin-bottom: 1rem;
      font-weight: 800;
    }

    .hero-section .lead {
      font-size: 1.3rem;
      opacity: 0.95;
      max-width: 800px;
      margin: 0 auto;
    }

    .hero-stats {
      display: flex;
      justify-content: center;
      gap: 40px;
      margin-top: 40px;
      flex-wrap: wrap;
    }

    .hero-stats .stat {
      text-align: center;
    }

    .hero-stats .stat h3 {
      font-size: 2.5rem;
      margin: 0;
      font-weight: 700;
    }

    .hero-stats .stat p {
      margin: 5px 0 0;
      opacity: 0.9;
    }

    .section {
      background: rgba(20, 27, 61, 0.6);
      border-radius: 20px;
      padding: 50px;
      margin-bottom: 40px;
      border: 1px solid rgba(139, 92, 246, 0.2);
    }

    .section h2 {
      color: #a78bfa;
      font-size: 2.2rem;
      margin-bottom: 1.5rem;
      border-bottom: 3px solid #667eea;
      padding-bottom: 15px;
    }

    .section h3 {
      color: #60a5fa;
      font-size: 1.8rem;
      margin-top: 2.5rem;
      margin-bottom: 1.2rem;
    }

    .section h4 {
      color: #c4b5fd;
      font-size: 1.4rem;
      margin-top: 2rem;
      margin-bottom: 1rem;
    }

    .methodology-box {
      background: linear-gradient(135deg, rgba(102, 126, 234, 0.15), rgba(118, 75, 162, 0.15));
      border-left: 5px solid #667eea;
      padding: 30px;
      border-radius: 12px;
      margin: 30px 0;
    }

    .formula {
      background: rgba(0, 0, 0, 0.3);
      padding: 20px 30px;
      border-radius: 12px;
      font-family: 'Courier New', monospace;
      font-size: 1.1rem;
      margin: 20px 0;
      border: 2px solid #667eea;
      text-align: center;
      color: #f0abfc;
    }

    .insight-box {
      background: linear-gradient(135deg, rgba(96, 165, 250, 0.12), rgba(139, 92, 246, 0.12));
      border-left: 5px solid #60a5fa;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
      font-size: 1.05rem;
    }

    .insight-box strong {
      color: #60a5fa;
    }

    .warning-box {
      background: linear-gradient(135deg, rgba(251, 146, 60, 0.15), rgba(239, 68, 68, 0.15));
      border-left: 5px solid #fb923c;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
    }

    .warning-box strong {
      color: #fb923c;
    }

    .success-box {
      background: linear-gradient(135deg, rgba(52, 211, 153, 0.15), rgba(16, 185, 129, 0.15));
      border-left: 5px solid #34d399;
      padding: 25px;
      border-radius: 12px;
      margin: 25px 0;
    }

    .success-box strong {
      color: #34d399;
    }

    .results-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin: 30px 0;
    }

    .metric-card {
      background: rgba(139, 92, 246, 0.1);
      border: 2px solid #667eea;
      padding: 25px;
      border-radius: 16px;
      text-align: center;
      transition: transform 0.3s, box-shadow 0.3s;
    }

    .metric-card:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 30px rgba(102, 126, 234, 0.4);
    }

    .metric-card h4 {
      color: #c4b5fd;
      font-size: 0.95rem;
      margin: 0 0 10px;
      text-transform: uppercase;
    }

    .metric-card .value {
      font-size: 2.5rem;
      font-weight: 800;
      color: #a78bfa;
    }

    .metric-card.best {
      border-color: #34d399;
      background: rgba(52, 211, 153, 0.15);
    }

    .metric-card.best .value {
      color: #34d399;
    }

    .image-wrapper {
      margin: 40px 0;
      border-radius: 16px;
      overflow: hidden;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5);
    }

    .image-wrapper img {
      width: 100%;
      display: block;
    }

    .image-caption {
      text-align: center;
      margin-top: 15px;
      font-style: italic;
      color: #9ca3af;
      font-size: 0.95rem;
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 30px;
      margin: 40px 0;
    }

    .step-number {
      display: inline-block;
      background: #667eea;
      color: white;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      text-align: center;
      line-height: 40px;
      font-weight: 700;
      margin-right: 15px;
      font-size: 1.2rem;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 30px 0;
      background: rgba(20, 27, 61, 0.4);
      border-radius: 12px;
      overflow: hidden;
    }

    thead {
      background: rgba(102, 126, 234, 0.3);
    }

    th,
    td {
      padding: 15px;
      text-align: left;
      border-bottom: 1px solid rgba(139, 92, 246, 0.2);
    }

    th {
      color: #a78bfa;
      font-weight: 600;
    }

    tr:hover {
      background: rgba(139, 92, 246, 0.1);
    }

    .process-flow {
      background: rgba(20, 27, 61, 0.6);
      border-radius: 16px;
      padding: 30px;
      margin: 30px 0;
    }

    .process-step {
      padding: 20px 0;
      border-left: 3px solid #667eea;
      padding-left: 30px;
      margin-left: 20px;
      position: relative;
    }

    .process-step::before {
      content: '';
      position: absolute;
      left: -8px;
      top: 25px;
      width: 13px;
      height: 13px;
      border-radius: 50%;
      background: #667eea;
      border: 3px solid #0a0e27;
    }

    code {
      background: rgba(139, 92, 246, 0.2);
      padding: 3px 8px;
      border-radius: 6px;
      color: #f0abfc;
      font-family: 'Courier New', monospace;
      font-size: 0.95em;
    }

    .nav-top {
      background: rgba(10, 14, 39, 0.95);
      padding: 20px 0;
      position: sticky;
      top: 0;
      z-index: 100;
      border-bottom: 1px solid rgba(139, 92, 246, 0.3);
    }

    .nav-top .container {
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .nav-top a {
      color: #a78bfa;
      text-decoration: none;
      transition: color 0.3s;
    }

    .nav-top a:hover {
      color: #c4b5fd;
    }
  </style>
</head>

<body>

  <!-- Navigation -->
  <nav class="nav-top">
    <div class="container">
      <a href="../index.html">‚Üê Apprentissage Supervis√©</a>
      <a href="../../index.html">üè† Accueil</a>
      <a href="../classification/index.html">Classification ‚Üí</a>
    </div>
  </nav>

  <!-- Hero Section -->
  <header class="hero-section">
    <div class="container">
      <h1>üìä Analyse de R√©gression Approfondie</h1>
      <p class="lead">
        Pr√©diction des ventes avec mod√®les lin√©aires, polynomiaux et r√©gularisation<br>
        <strong>Une approche m√©thodologique compl√®te du Machine Learning supervis√©</strong>
      </p>
      <div class="hero-stats">
        <div class="stat">
          <h3>51 235</h3>
          <p>Commandes analys√©es</p>
        </div>
        <div class="stat">
          <h3>6</h3>
          <p>Mod√®les explor√©s</p>
        </div>
        <div class="stat">
          <h3>0.943</h3>
          <p>R¬≤ final (Voting Ensemble)</p>
        </div>
        <div class="stat">
          <h3>$47</h3>
          <p>MAE optimal</p>
        </div>
      </div>
    </div>
  </header>

  <div class="container">

    <!-- Introduction & Context -->
    <section class="section">
      <h2>üéØ Contexte et Objectif du Projet</h2>

      <p style="font-size: 1.15rem;">
        La r√©gression est au c≈ìur de l'apprentissage supervis√© pour la pr√©diction de valeurs continues.
        Dans le contexte du <strong>Global Superstore</strong>, notre objectif est de construire un mod√®le
        capable de <strong>pr√©dire avec pr√©cision le montant des ventes (Sales)</strong> d'une commande en
        fonction de ses caract√©ristiques.
      </p>

      <div class="methodology-box">
        <h4 style="color: #667eea; margin-top: 0;">üî¨ M√©thodologie Scientifique</h4>
        <p>Notre approche suit une d√©marche rigoureuse en 5 √©tapes :</p>

        <div class="process-flow">
          <div class="process-step">
            <strong style="color: #a78bfa;">√âtape 1 : Pr√©paration des donn√©es</strong><br>
            Nettoyage, traitement des valeurs manquantes, encodage des variables cat√©gorielles, normalisation
          </div>
          <div class="process-step">
            <strong style="color: #a78bfa;">√âtape 2 : Baseline lin√©aire</strong><br>
            Construction d'un mod√®le de r√©gression lin√©aire simple pour √©tablir une r√©f√©rence de performance
          </div>
          <div class="process-step">
            <strong style="color: #a78bfa;">√âtape 3 : Impl√©mentation from scratch</strong><br>
            D√©veloppement de la descente de gradient pour comprendre les fondements math√©matiques
          </div>
          <div class="process-step">
            <strong style="color: #a78bfa;">√âtape 4 : S√©lection du degr√© polynomial optimal</strong><br>
            Utilisation du BIC pour √©viter le surapprentissage
          </div>
          <div class="process-step">
            <strong style="color: #a78bfa;">√âtape 5 : R√©gularisation Ridge</strong><br>
            Application de la r√©gularisation pour stabiliser le mod√®le final
          </div>
        </div>
      </div>

      <div class="insight-box">
        <strong>üí° Pourquoi la r√©gression est-elle cruciale pour le business ?</strong><br><br>
        Pr√©dire avec pr√©cision les ventes permet de :<br>
        ‚Ä¢ <strong>Optimiser les stocks</strong> en anticipant la demande<br>
        ‚Ä¢ <strong>Ajuster les prix</strong> et les remises de mani√®re strat√©gique<br>
        ‚Ä¢ <strong>Identifier les commandes √† forte valeur</strong> pour ciblage marketing<br>
        ‚Ä¢ <strong>Planifier les ressources logistiques</strong> (shipping, warehousing)
      </div>
    </section>

    <!-- Model 1: K-Nearest Neighbors (KNN) -->
    <section class="section">
      <h2><span class="step-number">1</span> K-Nearest Neighbors (KNN)</h2>

      <h3>üéØ Principe du KNN</h3>
      <p>
        Le K-Nearest Neighbors est un algorithme de r√©gression non-param√©trique qui pr√©dit les ventes
        en se basant sur la <strong>similarit√©</strong> avec les K commandes les plus proches dans l'espace des
        features.
      </p>

      <div class="methodology-box">
        <h4 style="color: #667eea; margin-top: 0;">üî¨ Fonctionnement du KNN</h4>
        <ul style="line-height: 2;">
          <li><strong>Distance euclidienne :</strong> Calcul de la distance entre chaque commande</li>
          <li><strong>S√©lection des K voisins :</strong> Identification des K observations les plus similaires</li>
          <li><strong>Moyenne pond√©r√©e :</strong> Pr√©diction = moyenne des ventes des K voisins</li>
          <li><strong>Optimisation de K :</strong> GridSearch pour trouver le nombre optimal de voisins</li>
        </ul>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/KNN Optimise - Valeurs Reelles vs Pr√©dictions.png" alt="KNN Optimis√© - Pr√©dictions" />
          <p class="image-caption">Figure 1 : KNN Optimis√© - Comparaison valeurs r√©elles vs pr√©dictions (R¬≤ ‚âà 0.89)</p>
        </div>
        <div class="image-wrapper">
          <img src="images/KNN_predictionVSreel.png" alt="KNN Pr√©diction vs R√©el" />
          <p class="image-caption">Figure 2 : Analyse d√©taill√©e des pr√©dictions KNN</p>
        </div>
      </div>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>R¬≤ Test</h4>
          <div class="value">0.89</div>
        </div>
        <div class="metric-card">
          <h4>MAE Test</h4>
          <div class="value">~$70</div>
        </div>
        <div class="metric-card">
          <h4>K Optimal</h4>
          <div class="value">5</div>
        </div>
        <div class="metric-card">
          <h4>Temps Train</h4>
          <div class="value">~0.5s</div>
        </div>
      </div>

      <div class="insight-box">
        <strong>‚úÖ Points forts du KNN :</strong><br>
        ‚Ä¢ <strong>Simple et intuitif</strong> : Pas d'hypoth√®ses sur la distribution des donn√©es<br>
        ‚Ä¢ <strong>Non-param√©trique</strong> : Capture naturellement les relations non-lin√©aires<br>
        ‚Ä¢ <strong>Performance solide</strong> : R¬≤ = 0.89 avec des hyperparam√®tres optimis√©s<br>
        ‚Ä¢ <strong>Interpr√©table</strong> : Les pr√©dictions sont bas√©es sur des exemples r√©els similaires
      </div>

      <div class="warning-box">
        <strong>‚ö†Ô∏è Limites du KNN :</strong><br>
        ‚Ä¢ <strong>Co√ªt de pr√©diction √©lev√©</strong> : Doit calculer la distance √† TOUTES les observations
        d'entra√Ænement<br>
        ‚Ä¢ <strong>Sensible √† la mal√©diction de la dimensionnalit√©</strong> : Performance d√©grad√©e avec beaucoup de
        features<br>
        ‚Ä¢ <strong>N√©cessite normalisation</strong> : Les features √† grande √©chelle dominent la distance
      </div>
    </section>

    <!-- Model 2: Random Forest -->
    <section class="section">
      <h2><span class="step-number">2</span> Random Forest ‚Äî Ensemble d'Arbres de D√©cision</h2>

      <h3>üå≤ Principe du Random Forest</h3>
      <p>
        Random Forest est un algorithme d'ensemble qui combine de nombreux arbres de d√©cision
        pour cr√©er un mod√®le robuste et performant. Chaque arbre est entra√Æn√© sur un sous-√©chantillon
        al√©atoire des donn√©es et features.
      </p>

      <div class="methodology-box">
        <h4 style="color: #667eea; margin-top: 0;">üîß Fonctionnement de Random Forest</h4>
        <ol style="line-height: 2;">
          <li><strong>Bootstrap Sampling :</strong> √âchantillonnage al√©atoire avec remplacement</li>
          <li><strong>Feature Randomness :</strong> S√©lection al√©atoire de features √† chaque split</li>
          <li><strong>Construction d'arbres :</strong> Chaque arbre est entra√Æn√© ind√©pendamment</li>
          <li><strong>Agr√©gation :</strong> Moyenne des pr√©dictions de tous les arbres</li>
          <li><strong>R√©duction variance :</strong> La moyenne r√©duit l'overfitting des arbres individuels</li>
        </ol>
      </div>

      <div class="image-wrapper">
        <img src="images/RandomForestTree.png" alt="Structure d'un arbre de d√©cision" />
        <p class="image-caption">Figure 3 : Visualisation d'un arbre de d√©cision dans Random Forest</p>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/RandomForest_regression.png" alt="Pr√©dictions Random Forest" />
          <p class="image-caption">Figure 4 : Pr√©dictions Random Forest vs valeurs r√©elles</p>
        </div>
        <div class="image-wrapper">
          <img src="images/RandomForestOptimise.png" alt="Random Forest Optimis√©" />
          <p class="image-caption">Figure 5 : Random Forest optimis√© avec GridSearch</p>
        </div>
      </div>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>R¬≤ Test</h4>
          <div class="value">0.88</div>
        </div>
        <div class="metric-card">
          <h4>MAE Test</h4>
          <div class="value">~$75</div>
        </div>
        <div class="metric-card">
          <h4>N_Estimators</h4>
          <div class="value">100</div>
        </div>
        <div class="metric-card">
          <h4>Temps Train</h4>
          <div class="value">~5s</div>
        </div>
      </div>

      <div class="success-box">
        <strong>üèÜ Avantages de Random Forest :</strong><br><br>
        ‚Ä¢ <strong>Robuste aux outliers</strong> : L'ensemble r√©duit l'impact des valeurs extr√™mes<br>
        ‚Ä¢ <strong>Feature importance</strong> : Identification automatique des variables les plus pr√©dictives<br>
        ‚Ä¢ <strong>Peu de tuning</strong> : Performances solides avec hyperparam√®tres par d√©faut<br>
        ‚Ä¢ <strong>Gestion du surapprentissage</strong> : La moyenne d'arbres r√©duit naturellement la variance
      </div>

      <div class="insight-box">
        <strong>üìä Insights M√©tier :</strong><br>
        Random Forest r√©v√®le que <strong>Quantity</strong>, <strong>Discount</strong> et <strong>Avg_Unit_Price</strong>
        sont les features les plus importantes pour pr√©dire les ventes. Cette information guide les strat√©gies
        de pricing et de promotion.
      </div>
    </section>

    <!-- Model 3: SVR (Support Vector Regression) -->
    <section class="section">
      <h2><span class="step-number">3</span> Support Vector Regression (SVR)</h2>

      <h3>‚ö° Principe du SVR</h3>
      <p>
        SVR utilise des <strong>fonctions noyau (kernel)</strong> pour projeter les donn√©es dans un espace
        de dimension sup√©rieure o√π une r√©gression lin√©aire devient possible. Le noyau RBF (Radial Basis Function)
        est particuli√®rement efficace pour capturer les relations non-lin√©aires.
      </p>

      <div class="formula">
        f(x) = Œ£(Œ±·µ¢ - Œ±·µ¢*)K(x·µ¢, x) + b
      </div>

      <p style="text-align: center; color: #9ca3af; margin-top: 10px;">
        o√π <code>K(x·µ¢, x)</code> est la fonction noyau (RBF, polynomial, etc.)
      </p>

      <div class="methodology-box">
        <h4 style="color: #667eea; margin-top: 0;">üîß Hyperparam√®tres Optimis√©s</h4>
        <ul style="line-height: 2;">
          <li><strong>Kernel :</strong> RBF (Radial Basis Function) pour capturer la non-lin√©arit√©</li>
          <li><strong>C (r√©gularisation) :</strong> Contr√¥le le compromis entre erreur d'entra√Ænement et marge</li>
          <li><strong>Epsilon (Œµ) :</strong> Tube de tol√©rance autour de la fonction de r√©gression</li>
          <li><strong>Gamma (Œ≥) : :</strong> Influence de chaque point d'entra√Ænement</li>
        </ul>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/SVR_comparaison_valeurs_prediction.png" alt="SVR - Pr√©dictions vs R√©el" />
          <p class="image-caption">Figure 6 : SVR - Comparaison valeurs r√©elles vs pr√©dictions</p>
        </div>
        <div class="image-wrapper">
          <img src="images/SVR_variance_explique.png" alt="SVR - Variance expliqu√©e" />
          <p class="image-caption">Figure 7 : Analyse de la variance expliqu√©e par SVR</p>
        </div>
      </div>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>R¬≤ Test</h4>
          <div class="value">0.87</div>
        </div>
        <div class="metric-card">
          <h4>MAE Test</h4>
          <div class="value">~$80</div>
        </div>
        <div class="metric-card">
          <h4>Kernel</h4>
          <div class="value" style="font-size: 1.5rem;">RBF</div>
        </div>
        <div class="metric-card">
          <h4>Temps Train</h4>
          <div class="value">~3s</div>
        </div>
      </div>

      <div class="insight-box">
        <strong>üéØ Performance SVR :</strong><br>
        ‚Ä¢ R¬≤ = 0.87 ‚Äî Bonnes performances gr√¢ce au noyau RBF<br>
        ‚Ä¢ <strong>Non-lin√©arit√© captur√©e</strong> : Le kernel transforme l'espace des features<br>
        ‚Ä¢ <strong>R√©gularisation automatique</strong> : Le param√®tre C √©vite l'overfitting<br>
        ‚Ä¢ Moins performant que Random Forest et Boosting sur ce dataset
      </div>

      <div class="warning-box">
        <strong>‚ö†Ô∏è Consid√©rations SVR :</strong><br>
        ‚Ä¢ <strong>Co√ªt computationnel</strong> : Complexit√© O(n¬≤) √† O(n¬≥) selon le kernel<br>
        ‚Ä¢ <strong>Sensible au scaling</strong> : N√©cessite absolument une normalisation des features<br>
        ‚Ä¢ <strong>Tuning d√©licat</strong> : C, epsilon et gamma doivent √™tre optimis√©s ensemble
      </div>
    </section>

    <!-- Model 4: XGBoost & LightGBM (Gradient Boosting) -->
    <section class="section">
      <h2><span class="step-number">4</span> XGBoost & LightGBM ‚Äî Gradient Boosting Optimis√©</h2>

      <h3>üöÄ Principe du Gradient Boosting</h3>
      <p>
        Le Gradient Boosting construit des arbres s√©quentiellement, chaque nouvel arbre corrigeant
        les erreurs des pr√©c√©dents. XGBoost et LightGBM sont des impl√©mentations optimis√©es avec
        r√©gularisation et parall√©lisation.
      </p>

      <div class="methodology-box">
        <h4 style="color: #667eea; margin-top: 0;">üîß Diff√©rences Cl√©s</h4>
        <ul style="line-height: 2;">
          <li><strong>XGBoost :</strong> Level-wise tree growth, r√©gularisation L1/L2, gestion native des valeurs
            manquantes</li>
          <li><strong>LightGBM :</strong> Leaf-wise growth (plus rapide), Gradient-based One-Side Sampling (GOSS)</li>
          <li><strong>Optimisation :</strong> GridSearch sur learning_rate, n_estimators, max_depth, subsample</li>
          <li><strong>Early stopping :</strong> Arr√™t automatique si performance stagne</li>
        </ul>
      </div>

      <h3>üéØ XGBoost ‚Äî √âtat de l'Art</h3>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/XGboostRegressor.png" alt="XGBoost Regressor" />
          <p class="image-caption">Figure 8 : XGBoost de base - Analyse des r√©sidus</p>
        </div>
        <div class="image-wrapper">
          <img src="images/XGboostOptimised.png" alt="XGBoost Optimis√©" />
          <p class="image-caption">Figure 9 : XGBoost optimis√© (R¬≤ ‚âà 0.92)</p>
        </div>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/XGboostGridSearchOptimise.png" alt="XGBoost GridSearch" />
          <p class="image-caption">Figure 10 : Performance XGBoost apr√®s GridSearch</p>
        </div>
        <div class="image-wrapper">
          <img src="images/XGBoost GridSearch Optimise - Valeurs Reelles vs Pr√©dictions.png"
            alt="XGBoost Pr√©dictions d√©taill√©es" />
          <p class="image-caption">Figure 11 : Valeurs r√©elles vs pr√©dictions XGBoost optimis√©</p>
        </div>
      </div>

      <h3>üí° LightGBM ‚Äî Vitesse et Performance</h3>

      <div class="image-wrapper">
        <img src="images/LightGBM_Robust.png" alt="LightGBM Robuste" />
        <p class="image-caption">Figure 12 : LightGBM robuste - Pr√©dictions vs ventes r√©elles</p>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/LightGBM Log-Transform.png" alt="LightGBM Log-Transform" />
          <p class="image-caption">Figure 13 : LightGBM avec log-transformation de la cible</p>
        </div>
        <div class="image-wrapper">
          <img src="images/CourbeValeurs Reelles vs Predictions (LightGBM Optimise).png" alt="Courbe LightGBM" />
          <p class="image-caption">Figure 14 : Courbe des pr√©dictions LightGBM optimis√©</p>
        </div>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/Courbe  Valeurs Reelles vs Predictions (LightGBM Optimise) - R2 = 0.9209.png"
            alt="LightGBM R¬≤ d√©taill√©" />
          <p class="image-caption">Figure 15 : LightGBM optimis√© avec R¬≤ = 0.9209</p>
        </div>
        <div class="image-wrapper">
          <img src="images/Scatter Plot R√©el vs Pr√©diction (LightGBM).png" alt="Scatter Plot LightGBM" />
          <p class="image-caption">Figure 16 : Scatter plot - Analyse des pr√©dictions LightGBM</p>
        </div>
      </div>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>XGBoost R¬≤</h4>
          <div class="value">0.920</div>
          <small style="color: #34d399;">Excellent</small>
        </div>
        <div class="metric-card best">
          <h4>LightGBM R¬≤</h4>
          <div class="value">0.921</div>
          <small style="color: #34d399;">√âtat-de-l'art</small>
        </div>
        <div class="metric-card">
          <h4>XGBoost MAE</h4>
          <div class="value">~$60</div>
        </div>
        <div class="metric-card">
          <h4>LightGBM Vitesse</h4>
          <div class="value">2x plus rapide</div>
        </div>
      </div>

      <div class="success-box">
        <h4 style="color: #34d399; margin-top: 0;">üèÜ Pourquoi XGBoost/LightGBM Dominent :</h4>
        <p><strong>1. R√©gularisation int√©gr√©e :</strong></p>
        <ul style="line-height: 1.8;">
          <li>P√©nalit√©s L1/L2 sur les poids des feuilles</li>
          <li>Contr√¥le de la complexit√© des arbres (max_depth, min_child_weight)</li>
          <li>√âvite l'overfitting naturellement</li>
        </ul>

        <p style="margin-top: 20px;"><strong>2. Feature engineering automatique :</strong></p>
        <ul style="line-height: 1.8;">
          <li>Capture automatiquement les interactions entre features</li>
          <li>Gestion native des valeurs manquantes</li>
          <li>Feature importance pour s√©lection de variables</li>
        </ul>

        <p style="margin-top: 20px;"><strong>3. Optimisations algorithmiques :</strong></p>
        <ul style="line-height: 1.8;">
          <li>Parall√©lisation sur CPU/GPU</li>
          <li>Cache-aware computation</li>
          <li>Histogram-based splitting (LightGBM)</li>
        </ul>
      </div>

      <div class="insight-box">
        <strong>üìä Comparaison XGBoost vs LightGBM :</strong><br><br>
        ‚Ä¢ <strong>Performances similaires</strong> : R¬≤ ‚âà 0.92 pour les deux<br>
        ‚Ä¢ <strong>LightGBM plus rapide</strong> : ~50% de r√©duction du temps d'entra√Ænement<br>
        ‚Ä¢ <strong>XGBoost plus stable</strong> : Meilleure robustesse sur petits datasets<br>
        ‚Ä¢ <strong>Production :</strong> LightGBM pr√©f√©r√© pour sa vitesse, XGBoost pour sa stabilit√©
      </div>
    </section>

    <!-- Model 5: Linear & Polynomial Regression -->
    <section class="section">
      <h2><span class="step-number">5</span> R√©gression Lin√©aire & Polynomiale</h2>

      <h3>üìê Base Lin√©aire ‚Äî Point de D√©part</h3>
      <p>
        La r√©gression lin√©aire simple √©tablit une relation lin√©aire entre features et cible.
        Bien que limit√©e, elle fournit une baseline interpr√©table et rapide.
      </p>

      <div class="formula">
        ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô
      </div>

      <div class="image-wrapper">
        <img src="images/linear_4plots.png" alt="Diagnostics r√©gression lin√©aire" />
        <p class="image-caption">Figure 17 : Diagnostics complets de la r√©gression lin√©aire (r√©sidus, QQ-plot, leverage)
        </p>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/Courbe des R√©sidus (Erreur = R√©el - Pr√©diction).png" alt="Courbe des r√©sidus" />
          <p class="image-caption">Figure 18 : Courbe des r√©sidus (Erreur = R√©el - Pr√©diction)</p>
        </div>
        <div class="image-wrapper">
          <img src="images/residual_distribution.png" alt="Distribution r√©siduelle" />
          <p class="image-caption">Figure 19 : Distribution des r√©sidus (test de normalit√©)</p>
        </div>
      </div>

      <h3>üßÆ Gradient Descent ‚Äî Impl√©mentation from Scratch</h3>
      <p>
        Pour comprendre les fondements de l'optimisation, nous avons impl√©ment√© la descente de gradient
        manuellement, d√©montrant une ma√Ætrise profonde des math√©matiques sous-jacentes.
      </p>

      <div class="formula">
        Œ∏ := Œ∏ - Œ± √ó ‚àáJ(Œ∏)
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/gd_convergence.png" alt="Convergence gradient descent" />
          <p class="image-caption">Figure 20 : Convergence de la descente de gradient</p>
        </div>
        <div class="image-wrapper">
          <img src="images/Gradient Descent Convergence - Learning Rate Selection.png" alt="Learning rate selection" />
          <p class="image-caption">Figure 21 : Impact du learning rate sur la convergence</p>
        </div>
      </div>

      <div class="insight-box">
        <strong>üéì Enseignements de l'impl√©mentation from scratch :</strong><br>
        ‚Ä¢ Learning rate optimal = 0.1 pour ce dataset<br>
        ‚Ä¢ Normalisation des features indispensable (StandardScaler)<br>
        ‚Ä¢ Notre impl√©mentation atteint R¬≤ ‚âà 0.68, validant notre compr√©hension math√©matique
      </div>

      <h3>üéØ Visualisation 3D ‚Äî Comprendre les Limitations</h3>

      <div class="image-wrapper">
        <img src="images/3d_plane.png" alt="Plan de r√©gression 3D" />
        <p class="image-caption">Figure 22 : Plan de r√©gression lin√©aire dans l'espace 3D (Quantity, Discount, Sales)
        </p>
      </div>

      <div class="formula">
        Sales ‚âà 28 + 71√óQuantity - 184√óDiscount
      </div>

      <div class="warning-box">
        <strong>‚ö†Ô∏è Limite du mod√®le lin√©aire :</strong><br>
        De nombreux points sont loin du plan, r√©v√©lant des <strong>relations non-lin√©aires</strong>
        que le mod√®le lin√©aire ne peut capturer ‚Üí n√©cessit√© d'enrichissement polynomial.
      </div>

      <h3>üìä S√©lection du Degr√© Polynomial Optimal (BIC)</h3>
      <p>
        Le Bayesian Information Criterion (BIC) aide √† choisir le degr√© polynomial optimal
        en p√©nalisant la complexit√© du mod√®le.
      </p>

      <div class="formula">
        BIC = n√óln(MSE) + k√óln(n)
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/BIC vs Polynomial Degree + Best Polynomial Fit (Degree = 8).png" alt="BIC vs Degr√©" />
          <p class="image-caption">Figure 23 : √âvolution du BIC selon le degr√© polynomial</p>
        </div>
        <div class="image-wrapper">
          <img src="images/bic_curve.png" alt="Courbe BIC" />
          <p class="image-caption">Figure 24 : Courbe BIC simplifi√©e</p>
        </div>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/Polynomial Degree 2 Smart Features Test.png" alt="Degr√© 2 optimal" />
          <p class="image-caption">Figure 25 : Degr√© 2 ‚Äî Ajustement optimal avec smart features</p>
        </div>
        <div class="image-wrapper">
          <img src="images/poly_degree8_fit.png" alt="Degr√© 8 overfit" />
          <p class="image-caption">Figure 26 : Degr√© 8 ‚Äî Overfitting catastrophique</p>
        </div>
      </div>

      <div class="image-grid">
        <div class="image-wrapper">
          <img src="images/degree_vs_performance.png" alt="Performance vs degr√©" />
          <p class="image-caption">Figure 27 : R¬≤ en fonction du degr√© polynomial</p>
        </div>
        <div class="image-wrapper">
          <img src="images/residuals_degree2.png" alt="R√©sidus degr√© 2" />
          <p class="image-caption">Figure 28 : Analyse des r√©sidus - Polynomial degr√© 2</p>
        </div>
      </div>

      <div class="results-grid">
        <div class="metric-card">
          <h4>Lin√©aire (Degr√© 1)</h4>
          <div class="value" style="font-size: 1.5rem;">R¬≤ = 0.673</div>
          <p style="color: #9ca3af; margin-top: 10px;">Baseline</p>
        </div>
        <div class="metric-card best">
          <h4>Polynomial (Degr√© 2)</h4>
          <div class="value" style="font-size: 1.5rem;">R¬≤ = 0.784</div>
          <p style="color: #34d399; margin-top: 10px;">OPTIMAL</p>
        </div>
        <div class="metric-card">
          <h4>Polynomial (Degr√© 3)</h4>
          <div class="value" style="font-size: 1.5rem;">R¬≤ = 0.801</div>
          <p style="color: #fb923c; margin-top: 10px;">D√©but overfit</p>
        </div>
        <div class="metric-card">
          <h4>Polynomial (Degr√© 8)</h4>
          <div class="value" style="font-size: 1.3rem;">R¬≤ = ‚àí67,495</div>
          <p style="color: #ef4444; margin-top: 10px;">Catastrophique</p>
        </div>
      </div>

      <div class="success-box">
        <h4 style="color: #34d399; margin-top: 0;">‚úÖ Pourquoi le Degr√© 2 est Optimal :</h4>
        <ul style="line-height: 1.8;">
          <li><strong>BIC comp√©titif</strong> : 633,180 ‚Äî bon √©quilibre complexit√©/performance</li>
          <li><strong>R¬≤ Test = 0.784</strong> : +16% vs lin√©aire, excellente g√©n√©ralisation</li>
          <li><strong>15 param√®tres seulement</strong> : Mod√®le parcimonieux et interpr√©table</li>
          <li><strong>Capture interactions quadratiques</strong> : Discount¬≤, Quantity√óDiscount sans instabilit√©s</li>
        </ul>
      </div>

      <div class="warning-box">
        <strong>‚ö†Ô∏è Le Pi√®ge du Degr√© 8 :</strong><br><br>
        Avec 495 param√®tres, le mod√®le <strong>m√©morise le bruit</strong> des donn√©es d'entra√Ænement
        et <strong>explose</strong> sur nouvelles donn√©es (R¬≤ n√©gatif = pire qu'une moyenne simple).
        Les courbes polynomiales cr√©ent des oscillations extr√™mes hors de la plage d'entra√Ænement.
      </div>
    </section>

    <!-- Model 6: Voting Regressor ‚Äî Ensemble Final -->
    <section class="section">
      <h2><span class="step-number">6</span> Voting Regressor ‚Äî Ensemble de Mod√®les üèÜ</h2>

      <h3>üó≥Ô∏è Principe de l'Ensemble Voting</h3>
      <p>
        Le Voting Regressor combine les pr√©dictions de plusieurs mod√®les h√©t√©rog√®nes pour
        produire une pr√©diction finale plus robuste et pr√©cise que n'importe quel mod√®le individuel.
      </p>

      <div class="methodology-box">
        <h4 style="color: #667eea; margin-top: 0;">üîß Architecture de l'Ensemble</h4>
        <ol style="line-height: 2;">
          <li><strong>Ridge Polynomial (degr√© 2)</strong> : Mod√®le lin√©aire enrichi avec features quadratiques</li>
          <li><strong>Random Forest</strong> : Ensemble d'arbres pour capturer les non-lin√©arit√©s</li>
          <li><strong>XGBoost</strong> : Gradient boosting avec r√©gularisation L1/L2</li>
          <li><strong>LightGBM</strong> : Boosting rapide avec leaf-wise growth</li>
        </ol>

        <p style="margin-top: 20px;">
          <strong>Agr√©gation :</strong> Moyenne pond√©r√©e ou simple des pr√©dictions de chaque mod√®le.
        </p>
      </div>

      <div class="formula">
        ≈∑_ensemble = (1/n) √ó Œ£(≈∑·µ¢) ou ≈∑_ensemble = Œ£(w·µ¢ √ó ≈∑·µ¢)
      </div>

      <div class="image-wrapper">
        <img src="images/VOTING REGRESSOR FINAL - 4 MOD√àLES + Avg_Unit_Price (R2 = 0.943+).png"
          alt="Voting Regressor Final" />
        <p class="image-caption">Figure 29 : Voting Regressor ‚Äî Ensemble de 4 mod√®les (R¬≤ = 0.943)</p>
      </div>

      <div class="image-wrapper">
        <img src="images/FINAL CHAMPION  Polynomial Ridge + Avg_Unit_Price (R2 = 0.9393).png" alt="Mod√®le Champion" />
        <p class="image-caption">Figure 30 : Champion ‚Äî Polynomial Ridge + Avg_Unit_Price (R¬≤ = 0.9393)</p>
      </div>

      <div class="results-grid">
        <div class="metric-card best">
          <h4>Voting Ensemble</h4>
          <div class="value">R¬≤ = 0.943</div>
          <small style="color: #34d399;">+140% vs baseline!</small>
        </div>
        <div class="metric-card best">
          <h4>Ridge + Avg_Unit_Price</h4>
          <div class="value">R¬≤ = 0.939</div>
          <small style="color: #34d399;">Tr√®s proche</small>
        </div>
        <div class="metric-card">
          <h4>MAE Ensemble</h4>
          <div class="value">~$47</div>
          <small style="color: #9ca3af;">Pr√©cision excellente</small>
        </div>
        <div class="metric-card">
          <h4>Temps Pr√©diction</h4>
          <div class="value">
            <100ms< /div>
              <small style="color: #9ca3af;">10K requ√™tes</small>
          </div>
        </div>

        <div class="success-box">
          <h4 style="color: #34d399; margin-top: 0;">üèÜ Pourquoi l'Ensemble Domine :</h4>

          <p><strong>1. Diversit√© des mod√®les :</strong></p>
          <ul style="line-height: 1.8;">
            <li>Ridge capture les relations lin√©aires et quadratiques</li>
            <li>Random Forest mod√©lise les interactions complexes</li>
            <li>XGBoost/LightGBM optimisent les erreurs r√©siduelles</li>
            <li>La combinaison r√©duit la variance et le biais simultan√©ment</li>
          </ul>

          <p style="margin-top: 20px;"><strong>2. Feature engineering Critique :</strong></p>
          <p>
            L'ajout de <strong>Avg_Unit_Price</strong> (Sales/Quantity) a √©t√© d√©terminant,
            produisant un bond de +15% en R¬≤ (de 0.784 √† 0.939). Cette feature capture
            le signal de profitabilit√© par unit√©.
          </p>

          <p style="margin-top: 20px;"><strong>3. Robustesse accrue :</strong></p>
          <ul style="line-height: 1.8;">
            <li>Moins sensible aux outliers qu'un mod√®le unique</li>
            <li>G√©n√©ralisation sup√©rieure sur donn√©es non vues</li>
            <li>Performance stable Across diff√©rents sous-ensembles de donn√©es</li>
          </ul>
        </div>

        <div class="insight-box">
          <strong>üíº Recommandations pour la Production :</strong><br><br>

          <strong>Option 1 - Performance Maximale :</strong><br>
          ‚Ä¢ <strong>Voting Regressor (R¬≤ = 0.943)</strong><br>
          ‚Ä¢ MAE ‚âà $47 ‚Üí Pr√©cision de ¬±$50 sur les ventes<br>
          ‚Ä¢ Id√©al pour: Planification strat√©gique, forecasting, d√©tection d'anomalies<br><br>

          <strong>Option 2 - Interpr√©tabilit√© :</strong><br>
          ‚Ä¢ <strong>Ridge + Polynomial + Avg_Unit_Price (R¬≤ = 0.939)</strong><br>
          ‚Ä¢ Mod√®le simple, rapide (<10ms), coefficients interpr√©tables<br>
            ‚Ä¢ Id√©al pour: Dashboards temps r√©el, API haute fr√©quence<br><br>

            <strong>Option 3 - √âquilibre :</strong><br>
            ‚Ä¢ <strong>LightGBM seul (R¬≤ = 0.921)</strong><br>
            ‚Ä¢ Vitesse maximale, feature importance native<br>
            ‚Ä¢ Id√©al pour: Applications mobiles, edge computing
        </div>

        <div class="warning-box">
          <strong>‚ö†Ô∏è Consid√©rations de D√©ploiement :</strong><br>
          ‚Ä¢ <strong>Monitoring continu</strong> : Surveiller le data drift (changements de distribution des
          features)<br>
          ‚Ä¢ <strong>Retraining p√©riodique</strong> : R√©entra√Æner tous les 3-6 mois avec nouvelles donn√©es<br>
          ‚Ä¢ <strong>A/B Testing</strong> : Comparer Voting vs mod√®le simple en production<br>
          ‚Ä¢ <strong>Fallback strategy</strong> : Avoir un mod√®le Ridge de secours si l'ensemble √©choue
        </div>
    </section>

    <!-- Comparison & Conclusion -->
    <section class="section">
      <h2>üìä Comparaison R√©capitulative et Conclusion</h2>

      <table>
        <thead>
          <tr>
            <th>Mod√®le</th>
            <th>R¬≤ Test</th>
            <th>MAE Test</th>
            <th>Temps Train</th>
            <th>Complexit√©</th>
            <th>Verdict</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Lin√©aire (Baseline)</td>
            <td>0.673</td>
            <td>$139.2</td>
            <td>~0.1s</td>
            <td>Faible</td>
            <td>R√©f√©rence solide</td>
          </tr>
          <tr>
            <td>Gradient Descent (scratch)</td>
            <td>0.680</td>
            <td>$119.8</td>
            <td>~2s</td>
            <td>Moyenne</td>
            <td>‚úÖ Validation p√©dagogique</td>
          </tr>
          <tr>
            <td>Polynomial Deg 2 + Ridge</td>
            <td>0.784</td>
            <td>$110.3</td>
            <td>~0.3s</td>
            <td>Optimale</td>
            <td>Baseline avanc√©e</td>
          </tr>
          <tr>
            <td>Random Forest</td>
            <td>0.88</td>
            <td>~$75</td>
            <td>~5s</td>
            <td>Moyenne-Haute</td>
            <td>Bonne performances</td>
          </tr>
          <tr>
            <td>KNN Optimis√©</td>
            <td>0.89</td>
            <td>~$70</td>
            <td>~0.5s</td>
            <td>Moyenne</td>
            <td>Tr√®s stable</td>
          </tr>
          <tr>
            <td>SVR (Kernel)</td>
            <td>0.87</td>
            <td>~$80</td>
            <td>~3s</td>
            <td>Moyenne</td>
            <td>Decent performances</td>
          </tr>
          <tr>
            <td>XGBoost Optimis√©</td>
            <td>0.920</td>
            <td>~$60</td>
            <td>~10s</td>
            <td>Haute</td>
            <td>Excellent compromis</td>
          </tr>
          <tr>
            <td>LightGBM</td>
            <td>0.920</td>
            <td>~$62</td>
            <td>~5s</td>
            <td>Haute</td>
            <td>Plus rapide que XGBoost</td>
          </tr>
          <tr style="background: rgba(52, 211, 153, 0.15);">
            <td><strong>Voting Ensemble</strong></td>
            <td><strong>0.943</strong></td>
            <td><strong>~$47</strong></td>
            <td><strong>~30s</strong></td>
            <td><strong>Tr√®s Haute</strong></td>
            <td><strong>üèÜ CHAMPION</strong></td>
          </tr>
          <tr style="background: rgba(52, 211, 153, 0.15);">
            <td><strong>Ridge + Avg_Unit_Price</strong></td>
            <td><strong>0.939</strong></td>
            <td><strong>~$50</strong></td>
            <td><strong>~0.5s</strong></td>
            <td><strong>Moyenne</strong></td>
            <td><strong>üèÜ Production</strong></td>
          </tr>
          <tr style="background: rgba(251, 146, 60, 0.15);">
            <td>Polynomial Deg 8</td>
            <td>‚àí67 495 ‚ùå</td>
            <td>N/A</td>
            <td>~1.5s</td>
            <td>Excessive</td>
            <td>Overfit catastrophique</td>
          </tr>
        </tbody>
      </table>

      <div class="success-box">
        <h3 style="color: #34d399; margin-top: 0;">üéì Synth√®se des Apprentissages</h3>

        <p><strong>1. M√©thodologie rigoureuse :</strong> Nous avons suivi une d√©marche scientifique compl√®te,
          du baseline au mod√®le optimis√©, en validant chaque √©tape par des diagnostics et m√©triques.</p>

        <p><strong>2. Compr√©hension profonde :</strong> L'impl√©mentation from scratch de la descente de gradient
          d√©montre une ma√Ætrise des fondamentaux math√©matiques, au-del√† de l'usage de biblioth√®ques.</p>

        <p><strong>3. Gestion du compromis biais-variance :</strong> Le choix du degr√© 2 (valid√© par BIC et
          performances test) illustre notre capacit√© √† √©viter le surapprentissage tout en capturant les
          non-lin√©arit√©s essentielles.</p>

        <p><strong>4. R√©gularisation strat√©gique :</strong> L'ajout de Ridge stabilise le mod√®le sans perte
          de performance, pr√©parant le terrain pour des ensembles futurs.</p>

        <p><strong>5. Valeur business :</strong> Une MAE de $110 est acceptable pour des d√©cisions op√©rationnelles
          (pricing, inventory) et constitue une base solide pour un mod√®le ensemble produisant R¬≤ > 0.96.</p>
      </div>

      <div class="insight-box">
        <strong>üöÄ Prochaines √âtapes :</strong><br><br>
        ‚Ä¢ <strong>Int√©gration dans un mod√®le ensemble</strong> (Voting, Stacking) avec Random Forest, XGBoost,
        LightGBM<br>
        ‚Ä¢ <strong>Feature engineering avanc√©</strong> : agr√©gations temporelles, encodages cat√©goriels optimis√©s<br>
        ‚Ä¢ <strong>D√©ploiement en production</strong> via API Flask/FastAPI pour pr√©dictions en temps r√©el<br>
        ‚Ä¢ <strong>Monitoring continu</strong> des performances pour d√©tecter le data drift
      </div>
    </section>

  </div>

  <!-- Footer -->
  <footer
    style="background: #0a0e27; border-top: 1px solid rgba(139, 92, 246, 0.3); padding: 40px 0; text-align: center; margin-top: 80px;">
    <div class="container">
      <p style="color: #9ca3af; font-size: 0.95rem;">
        ¬© 2025 Global Superstore ‚Äî Analyse de R√©gression Approfondie<br>
        <span style="color: #667eea;">Machine Learning Portfolio Project</span>
      </p>
    </div>
  </footer>

</body>

</html>